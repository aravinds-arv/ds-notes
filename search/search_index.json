{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#home","title":"Home","text":"<p>Experimenting stuff \u2697\ufe0f.. check back later!</p> SourceResult <pre><code>print(\"Hello world!\")\n</code></pre> <p>Hello world!</p>"},{"location":"foundational/","title":"Foundational Level","text":""},{"location":"foundational/maths_2/week_1/1_vectors/","title":"Vectors","text":""},{"location":"foundational/maths_2/week_1/1_vectors/#vectors","title":"Vectors","text":""},{"location":"foundational/maths_2/week_1/1_vectors/#why-linear-algebra","title":"Why Linear Algebra?","text":"<p>Linear Algebra is a field of mathematics that could be said to lie in the intersection of many different engineering domains some of which include, but not limited to data science, structural engineering, image processing, linear programming, fluid mechanics, control theory, network flow &amp; engineering design. Many of these areas of science and engineering require numerical methods, and linear algebra provides the vast majority of these numerical tools. </p> <p>But why is this the case?, what makes this specific domain of mathematics so versatile? Well to put it simply we could say that linear algebra is the most easiest and most practical language we can use to convey data about real world to computers so that they can process large amounts of data and provide us with useful insights. </p> <p>Finally speaking in the context of data science, it is quite universally accepted that a solid grasp of the concepts in linear algebra lays foundation for a deeper understanding of data science and ML algorithms. As we'll see in later courses, many of these so called algorithms are indeed powered by methods from linear algebra behind the scenes. <sup>1</sup></p>"},{"location":"foundational/maths_2/week_1/1_vectors/#vectors_1","title":"Vectors","text":"<p>As we said above, linear algebra evolved as way of reliably conveying information about the real world in a language that computers can easily understand and vectors could be thought of as the symbols or alphabets that make up this language. In a broader sense there are atleast three different ways to think about vectors:</p> <ul> <li>Vectors as an array of numbers - The computer science perspective</li> <li>Vectors as an arrow with a magnitude and direction - The physics perspective</li> <li>And vectors as objects that can added together or scaled or both - The mathematics perspective</li> </ul> <p>For our purposes, we'll most often be using the mathematics (or numerical) view of vectors because this view generalizes both of the other views, saying that a vector can be anything where there\u2019s a sensible notion of adding two objects or scaling an object by some factor to produce another object of the same kind. From this mathematical point of view anything that satisfies these two properties can be considered as a vector. Here are some examples of such vector objects:</p> <ul> <li>Geometric vectors: This example of a vector may be familiar from highschool physics. What defines a geometric vector is its length and the direction it\u2019s pointing, but as long as those two facts are the same you can move it around and it\u2019s still the same vector. Physical quanities like velocity, displacement and force are represented using geometric vectors.</li> </ul> <ul> <li>Polynomials: Polynomials are also vectors, two polynomials can be added together which result in another polynomial and they can be  multiplied by a scalar to give a polynomial as well. Therefore, polynomials are (rather unusual) instances of vectors.</li> </ul> <ul> <li>Audio Signals: Audio signals are vectors, they are represented as a series of numbers. We can add audio signals together, and their sum is a new audio signal. If we scale an audio signal, we also get an audio signal. Therefore, audio signals are a type of vector too.</li> <li>Elements of \\(\\mathbb{R}^n\\): This is the type of vectors that we'll primarily focus on. Elements of \\(\\mathbb{R}^n\\) are tuples or arrays of \\(n\\) real numbers written vertically with square brackets around them. For example:-</li> </ul> \\[ a = \\begin{bmatrix}     -2 \\\\     3 \\\\ \\end{bmatrix} \\in \\mathbb{R}^2 \\] <p>This is a vector in 2-dimensions since is has two components -2 and 3. Geometrically we represent it as an arrow in cartesian plane with it's tail sitting at the origin and it's tip at the coordinates \\((-2, 3)\\).</p> <p>Note</p> <p>Keep in mind that the coordinate \\((-2,3) \\neq \\begin{bmatrix} -2 \\\\ 3 \\\\ \\end{bmatrix}\\). The former is an indicator of position in space while the latter represents a real mathematical object.</p> <p>This geometric intuition of a vector could be extended to higher dimension as well (though we may not really be able to visualize 4-d space or beyond), for example consider a vector in 3-d space \\(a = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\).</p> <p>The first number \\(2\\) tells you how far to move along the x-axis, the second number \\(1\\) tells you how far to move parallel to the y-axis and the third number \\(3\\) tells you how far to move parallel to the z-axis.</p>"},{"location":"foundational/maths_2/week_1/1_vectors/#vector-operations","title":"Vector Operations","text":"<p>We said earlier that any object that can be added to another or scaled by some factor to produce another object of the same kind could be called as a vector. Pretty much every linear algebra topic revolves around these two fundamental operations of vector addition and scalar multiplication. Now that we have a good idea of what a vector is both numerically and geometrically let's look at how we can perform these fundamental operations on them.</p>"},{"location":"foundational/maths_2/week_1/1_vectors/#vector-addition","title":"Vector Addition","text":"<p>To add two vectors we simply take the sum of their corresponding components. Here's an example:</p> \\[ \\begin{bmatrix}     1 \\\\     2 \\\\ \\end{bmatrix} +  \\begin{bmatrix}     3 \\\\     -1 \\\\ \\end{bmatrix} = \\begin{bmatrix}     1 + 3 \\\\     2 - 1 \\\\ \\end{bmatrix} =  \\begin{bmatrix}     4 \\\\     1 \\\\ \\end{bmatrix} \\] <p>How do we represent this geometrically? To obtain the sum of two vectors in space, we move the first vector so that it's tail sits on the tip of the second vector and then join the tail of the first vector to the tip of the second:</p> <p></p>"},{"location":"foundational/maths_2/week_1/1_vectors/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>A scalar is just a number or a factor by which we scale any given vector, such an operation is called scalar multiplication. To perform scalar multiplication, we again simply multiply each component by the number or scalar. Example:</p> \\[ 2 \\times \\begin{bmatrix}     3 \\\\     1 \\\\ \\end{bmatrix} = \\begin{bmatrix}     6 \\\\     2 \\\\ \\end{bmatrix} \\] <p>And geometrically this would literally just mean to scale or extend the vector by the given factor:</p> <p>In case the factor that is multiplies is either between \\(0\\) and \\(1\\) or is less than \\(0\\) then, the vector is squished down or flipped around and extended in the opposite direction</p> <p></p> <p>To get a better idea of the geometric intuitions behind these vector operations and why they work so, I'd highly recommend you to watch the 3Blue1Brown video in the recommended watch section below.</p>"},{"location":"foundational/maths_2/week_1/1_vectors/#recommended-watch","title":"Recommended Watch","text":"<ol> <li> <p>If you'd like to get a better idea about how different concepts in linear algebra is used in data science and machine learning click here \u21a9</p> </li> </ol>"},{"location":"foundational/maths_2/week_1/2_matrices/","title":"Matrices","text":""},{"location":"foundational/maths_2/week_1/2_matrices/#matrices","title":"Matrices","text":"<p>Like vectors, matrices also play a central role in linear algebra. Matrices when paired with vectors can be used to compactly represent linear systems of equations as we'll see in the next lesson. Matrices can also be viewed as linear transformations. But before diving into any of these interesting topics let's first formally define what a matrix is and what kind of operations we can perform on it.</p> <p>Defintion: A real valued matrix \\(A\\) of order \\(m\\times n\\) is an \\(m\\cdot n\\) tuple of real elements \\(a_{ij}\\) where \\(i = 1,2,...,m\\) and \\(j = 1,2,...,n\\) and is ordered according to a rectangular scheme consisting of \\(m\\) rows and \\(n\\) columns.</p> \\[ \\left.\\left[ \\vphantom{\\begin{array}{c}1\\\\1\\\\1\\\\1\\\\1\\end{array}} \\smash{\\underbrace{     \\begin{array}{cccc}     a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\     a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{1n} \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\\\ \\end{array}         }_{n\\text{ columns}}} \\right]\\right\\} \\,m\\text{ rows} \\] <p>Vectors are also in a sense, a special case of matrices where the order is either \\(1\\times n\\) (row vectors) or \\(m\\times 1\\) (column vectors).</p>"},{"location":"foundational/maths_2/week_1/2_matrices/#matrix-operations","title":"Matrix Operations","text":"<p>Similar to vectors we have certain operations that can be performed on matrices to combine two or more matrices and obtain other matrices. These operations include:</p> <ul> <li>Matrix Addition</li> <li>Matrix Multiplication</li> <li>Scalar Multiplication</li> <li>Transposition</li> <li>Inversion</li> </ul> <p>Let's go over each of these in detail!</p>"},{"location":"foundational/maths_2/week_1/2_matrices/#1-matrix-addition","title":"1. Matrix Addition","text":"<p>The addition of matrices is one of the basic operations that is performed on matrices. Two or more matrices of the same order can be added by adding the corresponding elements of the matrices. If \\(A = [a_{ij}]\\) and \\(B = [b_{ij}]\\) are two matrices of the same order, that is, they have the same number of rows and columns, then the addition of matrices A and B is simply given by \\(A+B = [a_{ij}] + [b_{ij}] = [a_{ij}+b_{ij}]\\).</p> \\[ \\scriptsize{ \\begin{bmatrix}     a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\     a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{1n} \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\\\ \\end{bmatrix} + \\begin{bmatrix}     b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1n} \\\\     b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{1n} \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     b_{m1} &amp; a_{m2} &amp; \\cdots &amp; b_{mn} \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix}     a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \\cdots &amp; a_{1n} + b_{1n} \\\\     a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2n} + b_{2n} \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \\cdots &amp; a_{mn} + b_{mn} \\\\ \\end{bmatrix} } \\] <p>Matrix addition has the following properties most of which are similar to addition of two numbers:</p> <ul> <li>Commutativity: \\(A+B=B+A\\)</li> <li>Associativity: \\(A+(B+C)=(A+B)+C\\)</li> <li>Additive Identity: \\(A+0=A=0+A\\), here \\(0\\) represents the zero matrix, a matrix filled with zeros in all it's position. When any matrix \\(A\\) is added to \\(0\\), the resultant matrix is \\(A\\) itself</li> <li>Additive Inverse: \\(A + {-A}=0=A + {-A}\\), where \\({-A}\\) is the additive inverse of \\(A\\) ie., if \\(A=[a_{ij}]\\), then \\({-A}=[{-a_{ij}}]\\)</li> </ul>"},{"location":"foundational/maths_2/week_1/2_matrices/#2-matrix-multiplication","title":"2. Matrix Multiplication","text":"<p>Unlike addition, multiplication of two matrices is not defined as an element-wise operation, ie., \\(c_{ij} \\neq a_{ij} \\times b_{ij}\\) <sup>1</sup> where \\(c_{ij}\\) represents the elements in the product matrix \\(C=AB\\). Instead to compute the elements of the resultant matrix, \\(C\\), we multiply the elements in the rows of \\(A\\) with the elements in the columns of \\(B\\) and take the summation of these products. This can be more clearly understood from the below multiplication of two matrices of order 3 x 3.</p> \\[ \\small{ \\underbrace{ \\begin{bmatrix}     \\mathrm{a} &amp; \\mathrm{b} &amp; \\mathrm{c} \\\\     \\mathrm{d} &amp; \\mathrm{e} &amp; \\mathrm{f} \\\\     \\mathrm{g} &amp; \\mathrm{h} &amp; \\mathrm{i} \\\\ \\end{bmatrix}}_{3\\times 3} \\cdot \\underbrace{ \\begin{bmatrix}     \\mathrm{j} &amp; \\mathrm{k} &amp; \\mathrm{l} \\\\     \\mathrm{m} &amp; \\mathrm{n} &amp; \\mathrm{o} \\\\     \\mathrm{p} &amp; \\mathrm{q} &amp; \\mathrm{r} \\\\ \\end{bmatrix}}_{3\\times 3} = \\underbrace{ \\begin{bmatrix}     \\mathrm{(aj + bm + cp)} &amp; \\mathrm{(ak + bn cq)} &amp; \\mathrm{(al + bo + cr)} \\\\     \\mathrm{(dj + em + fp)} &amp; \\mathrm{(dk + en + fq)} &amp; \\mathrm{(dl + eo + fr)} \\\\     \\mathrm{(gj + hm + ip)} &amp; \\mathrm{(gk + hn + iq)} &amp; \\mathrm{(gl + ho + ir)} \\\\ \\end{bmatrix}}_{3\\times 3} } \\] <p>So for obtaining some element \\(c_{ij}\\) we multiply the \\(i\\)th row of \\(A\\) with the \\(j\\)th column of \\(B\\) and sum them up. This is also termed as the dot product of the corresponding row and column. </p> <p>Note that here while we have both \\(A\\) and \\(B\\) of the same order (\\(3\\times 3\\)), this might not always be the case, sometimes we might need to multiply matrices with different shapes. In such cases matrices can only be multiplied if their neighbouring dimensions match ie., the number of columns in \\(A\\) should be the same as the number of rows in \\(B\\). For instance for two matrices \\(A_{(m\\times k)}\\) and \\(A_{(k\\times n)}\\), the product \\(AB\\) is possible but the reverse \\(BA\\) is not allowed.</p> \\[ \\underbrace{A}_{m\\times k} \\underbrace{B}_{k\\times n} = \\underbrace{C}_{m\\times n} \\] <p>The resultant matrix \\(AB\\) would thus be of order \\(m\\times n\\).</p> <p>Matrix addition has the following properties:</p> <ul> <li>Not Commutative: \\(AB\\neq BA\\)</li> <li>Associativity: \\(A(BC)=(AB)C\\)</li> <li>Multiplicative Identity: \\(AI=A=IA\\), here \\(I\\) represents an identity matrix of suitable order</li> </ul> <p>Identity Matrix</p> <p>An identity matrix of order \\(n\\times x\\) is a matrix having \\(1\\)'s as it's diagonal entries and \\(0\\)'s everywhere else. For instance here's an identity matrix of order \\(3\\times 3\\):</p> \\[ \\begin{bmatrix}     1 &amp; 0 &amp; 0 \\\\     0 &amp; 1 &amp; 0 \\\\     0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\] <ul> <li>Distributivity: \\(A(B+C) = AB + AC\\)</li> </ul>"},{"location":"foundational/maths_2/week_1/2_matrices/#3-scalar-multiplication","title":"3. Scalar Multiplication","text":"<p>Scalar muliplication is the easiest among the matrix operations listed here. Basically for some scalar  \\(\\lambda \\in \\mathbb{R}\\) and some matrix \\(A\\), their product \\(\\lambda A\\) simply represents the matrix with each element in \\(A\\) scaled (or multiplied) by the factor \\(\\lambda\\).</p> \\[ \\lambda \\begin{bmatrix}     a_{11} &amp; a_{12} \\\\     a_{21} &amp; a_{22} \\\\ \\end{bmatrix} = \\begin{bmatrix}     \\lambda a_{11} &amp; \\lambda a_{12} \\\\     \\lambda a_{21} &amp; \\lambda a_{22} \\\\ \\end{bmatrix} \\] <p>The following properties hold for scalar multiplication of matrices:</p> <ul> <li>Associativity: \\(\\lambda (AB) = (\\lambda A)B = A(\\lambda B)\\)</li> <li>Distributivity: \\(\\lambda(A + B) = \\lambda A + \\lambda B\\)</li> </ul>"},{"location":"foundational/maths_2/week_1/2_matrices/#4-transposition","title":"4. Transposition","text":"<p>Matrix transposition of finding the transpose of some matrix is an interesting operation. We simply write the rows of the matrix as columns or the columns as rows. For some matrix \\(A\\), we denote it's transpose as \\(A^T\\).</p> \\[ \\begin{align}   A &amp;= \\begin{bmatrix}     1 &amp; 3 \\\\     2 &amp; 4 \\\\     3 &amp; 5 \\\\ \\end{bmatrix} \\\\\\\\ \\implies A^T &amp;= \\begin{bmatrix}     1 &amp; 2 &amp; 3 \\\\     3 &amp; 4 &amp; 5 \\\\ \\end{bmatrix}   \\end{align} \\] <p>Notice that each element \\(a_{ij}\\) in \\(A\\) has moved to the position \\(a_{ji}\\) in \\(A^T\\). Also note that while \\(A\\) is a \\(3\\times 2\\) matrix, \\(A^T\\) is of order \\(2\\times 3\\). </p> <p>Pause and Ponder</p> <p>What do you think would happen to the order of a square matrix when it is transposed? And what would happen to it's diagonal elements (elements \\(a_{ij}\\) where \\(i=j\\))?</p> <p>Properties of transposition:</p> <ul> <li>\\((A^T)^T = A\\)</li> <li>\\((\\lambda A)^T = \\lambda A^T\\)</li> <li>\\((A + B)^T = A^T + B^T\\)</li> <li>\\((AB)^T = B^TA^T\\)</li> <li>If \\(A=A^T\\), then \\(A\\) is said to be symmetric</li> </ul>"},{"location":"foundational/maths_2/week_1/2_matrices/#5-inversion","title":"5. Inversion","text":"<p>The inverse of a matrix \\(A\\) is another matrix that when multiplied with \\(A\\) on either side results in an identity matrix. It is denoted as \\(A^{-1}\\), so mathematically we have:</p> \\[ AA^{-1} = A^{-1}A = I \\] <p>Unfortunately, not all matrices possess an inverse. A matrix is said to be invertible if and only if it is a square matrix and it has a non-zero determinant - a topic that we'll see in a later lesson. And in this lesson we'll not be looking at the general way of computing inverses instead now, we'll look at a simpler way of computing the inverse of a general \\(2\\times 2\\) matrix. Consider the below matrix \\(A\\):</p> \\[ A = \\begin{bmatrix}     a &amp; b \\\\     c &amp; d \\\\ \\end{bmatrix} \\] <p>Suppose we multiply another matrix A' of the form:</p> \\[ A' = \\begin{bmatrix}     d &amp; -b \\\\     -c &amp; a \\\\ \\end{bmatrix} \\] <p>then we get</p> \\[ \\begin{align}     A'A = \\begin{bmatrix}         ad - bc &amp; 0 \\\\         0 &amp; ad - bd \\\\     \\end{bmatrix} &amp;= (ad-bc)I \\\\\\\\     \\implies      \\frac{1}{ad-bc} A' A &amp;= I \\end{align} \\] <p>Comparing this equation with \\(A^{-1}A = 1\\), we get</p> \\[ A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix}     d &amp; -b \\\\     -c &amp; a \\\\ \\end{bmatrix} \\] <p>Therefore \\(A^{-1}\\) can only exist if \\(ad-bc \\neq 0\\), this is what we meant by a non-zero determinant. Matrices that have a non-zero determinant value (and thus invertible) are also termed as non-singular matrices, we'll revisit these topics in more detail in upcoming lessons.</p>"},{"location":"foundational/maths_2/week_1/2_matrices/#matrices-as-linear-transformation","title":"Matrices as Linear Transformation","text":"<p>We saw how we can geometrically represent vectors as arrows with a direction and magitude in the last lesson. But what about matrices? How can we think about them in a geometrically? In a geometric sense, matrices can be viewed as linear transformations. Without getting too much into the weeds let's try to develop a brief understanding of this more intuitive picture of matrices (do watch the videos attatched below to get a better idea of this topic).</p> <p>Matrices can be thought of as functions which takes as input a vector and produces as output another vector. That's why we call this type of an operation a transformation, they transform one object to another.</p> <p>So how does a matrix encode the information about how a vector needs to be transformed from one vector from one to another? They do so by recording the position of the basis vectors of a vector space after the transformation.</p> <p>Basis Vectors</p> <p>Basis vectors are a special set of linearly independent vectors that can be used to represent any other vector within that vector space. The standard basis vectors are considered to be the orthogonal unit vectors. For example for a 2-d vector space, the standard basis would be the unit vector along x-axis, \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}\\) and the unit vector along y-axis, \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}\\):</p> <p> </p> <p>Let's try to visualize a 2-d tranformation to get a better hold of this idea. If we think about the 2-d vector space as a Cartesian plane with grid lines spaced by unit length, then here's what a linear transformation would look like:</p> <p>Now if had initially had some vector say \\(v=\\begin{bmatrix} -1 \\\\ 2 \\\\ \\end{bmatrix}\\), that was initially described as a linear combination of the standard basis vectors ie., \\(v = -1i + 2j\\), then after the tranformation we can get the position of the tranformed vector \\(Lv\\) by simply taking the same linear combination of the new basis vectors.</p> <p>In case you noticed, yes the transformation shown in the previous animation is not the same as the one  above (because I didn't create it, 3b1b did!), but still I hope you get the idea. </p> <p>The only information we need to preserve in a matrix to retrieve any vector in the transformed vector space is the transformed basis vectors and that's precisely what the columns of a matrix represent. In the above transformation, we have the transformed bases as:</p> \\[ \\begin{align}     L(i) = \\begin{bmatrix}         1 \\\\ -2 \\\\     \\end{bmatrix} \\quad     L(j) = \\begin{bmatrix}         3 \\\\ 0 \\\\     \\end{bmatrix} \\end{align} \\] <p>Now using these 4 numbers how can we get the transformed version of \\(v\\)? We simply multiply the matrix having these transformed bases as it's columns with \\(v\\) (the order of multiplication here should by matrix \\(\\times\\) vector):</p> \\[ \\begin{bmatrix} 1 &amp; 3 \\\\ -2 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix}     -1 \\\\ 2 \\\\ \\end{bmatrix} =  \\begin{bmatrix}     5 \\\\ 2 \\\\ \\end{bmatrix} \\] <p>Isn't that beautiful, with just 4 numbers we can now tell us where any vector in a vector space would land after a transformation. Now whenever we encounter a matrix we should learn to immediately associate it with it's geometric picture - a transformation of vector space.</p> <p>If you didn't quite understand the idea of linear transformation now, don't worry, this intuition behind matrices though helpful if learnt (and taught) earlier, is not necessarily needed to understand the topics ahead (atleast upto week 3). We'll formally be introduced to topics like vector spaces, basis vectors etc. around week 4. And once you have a good idea of these topics you can revisit this lesson and try to connect the dots.</p>"},{"location":"foundational/maths_2/week_1/2_matrices/#recommended-watches","title":"Recommended Watches","text":"<ol> <li> <p>This kind of element-wise multiplication often appears in programming languages when we multiply (multi-dimensional) arrays with each other, and is called a Hadamard product.\u00a0\u21a9</p> </li> </ol>"},{"location":"foundational/maths_2/week_1/3_linear_systems/","title":"Linear System of Equations","text":""},{"location":"foundational/maths_2/week_1/4_determinants/","title":"Determinants","text":""},{"location":"foundational/maths_2/week_1/4_determinants/#determinants","title":"Determinants","text":"<p>Determinants are important concepts in linear algebra. It is used in solving linear systems of equations, testing the invertibility of matrices, finding the eigenvalues &amp; eigenvectors and more. It is only defined for square matrices of the form \\(A^{n \\times{n}}\\) and is usually denoted by \\(\\det(A)\\) or \\(|A|\\).</p> <p>Definition: The determinant of a square matrix \\(A \\in \\mathbb{R}^{n \\times{n}}\\) is a function (of it's entries) that maps \\(A\\) onto a real number.</p> <p>Geometrically speaking determinants represent the factor by which a region of space is scaled by a linear transformation (represented by a matrix). For a two dimensional linear transformation this would be the factor by which a plane of unit area gets scaled to ie., either enlarged or shrunk after applying the transformation.</p> <p></p> <p>As you can see above the area is scaled by a factor of 6 on applying the linear transformation represented by the matrix. Similarly for 3 dimensions the determinants provide the factor by which volumes are scaled or simply the volume of a 3-dimesional parallelepiped (since we're a scaling up or down from a region of unit volume).</p> A parellelepiped in 3 dimensions <p>This intuition extends to higher dimensions. Generally we could say that the determinant of a matrix \\(A\\) represents the signed volume of the n-dimensional parallelepiped formed by the column vectors of \\(A\\). The sign of the determinant indicates the orientation of the basis vectors in space. For example in the previous set of images a -ve determinant would swap the positions of the red and the green basis vectors much similar to flipping a sheet of paper. And a determinant of value 0 would mean all of space has been squished to form a region of lower dimension, a plane in case of 3-d tranformations and a line in case of 2-d tranformations.</p>"},{"location":"foundational/maths_2/week_1/4_determinants/#recommended-watch","title":"Recommended watch","text":""},{"location":"foundational/maths_2/week_1/5_computing_determinant/","title":"Computing Determinants","text":""},{"location":"foundational/maths_2/week_1/5_computing_determinant/#computing-determinants","title":"Computing Determinants","text":"<p>We have explicit explicit expressions to compute the determinants in terms of the elements of a matrix only for \\(1 \\times{1}\\) and \\(2 \\times{2}\\) matrices. For n = 1,</p> <p>\\[\\det(A) = \\det([a_{11}]) = a_{11}\\]</p> <p>For n = 2,</p> <p>\\[\\det(A) =  \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{vmatrix} = ad - bc \\]</p> <p>For matrices of higher orders \\(A \\in \\mathbb{R}^{n \\times{n}}\\), we recursively try to reduce the problem of finding the determinant of the \\(n \\times{n}\\) matrix to finding the determinant of (\\(n-1) \\times(n-1)\\) matrices until the problem gets reduced to computing the determinants of (many) simple \\(2 \\times{2}\\) matrices. This algorithm of finding the determinant of any general square matrix is called the Laplace Expansion. For a matrix \\(A \\in \\mathbb{R}^{n \\times{n}}\\), Laplace expansion can either be done along rows or along columns:</p> <p>1. Expansion along row i</p> <p>\\[\\det(A) = \\sum_{j=1}^{n} (-1)^{(i + j)} a_{ij} \\times \\det(A_{i,j})\\]</p> <p>2. Expansion along column j</p> <p>\\[\\det(A) = \\sum_{i=1}^{n} (-1)^{(i + j)} a_{ij} \\times \\det(A_{i,j})\\]</p> <p>Here \\(A_{i,j}\\) is the submatrix of \\(A\\) that is obtained on deleting the \\(i^{th}\\) row and the \\(j^{th}\\) column. The determinant of this matrix \\(\\det(A_{i,j})\\) is called the minor of the term \\(a_{ij}\\) and is usually denoted by \\(M_{i,j}\\). And when the minor is multiplied by the term \\((-1)^{(i+j)}\\) it's called the cofactor of \\(a_{ij}\\) denoted by \\(C_{i,j}\\).</p>"},{"location":"foundational/maths_2/week_1/5_computing_determinant/#example","title":"Example","text":"<p>Let us try to compute the determinant of</p> <p>\\[A = \\begin{vmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{vmatrix} \\]</p> <p>Using the Laplace expansion along the first row we get</p> <p>\\[\\det(A) = (-1)^{(1+1)} . 1\\begin{vmatrix} 1 &amp; 2 \\\\ 0 &amp; 1 \\\\ \\end{vmatrix} + (-1)^{(1+2)} . 2\\begin{vmatrix} 3 &amp; 2 \\\\ 0 &amp; 1 \\\\ \\end{vmatrix} + (-1)^{(1+3)} . 3\\begin{vmatrix} 3 &amp; 1 \\\\ 0 &amp; 0 \\\\ \\end{vmatrix} \\]</p> <p>Now we can apply the determinant formula for \\(2\\times2\\) matrices and obtain the final result</p> <p>\\[\\det(A) = 1(1-0) - 2(3-0) + 3(0-0) = -5\\]</p>"},{"location":"foundational/maths_2/week_1/6_properties_determinants/","title":"Properties of Determinants","text":""},{"location":"foundational/maths_2/week_1/6_properties_determinants/#properties-of-determinants","title":"Properties of Determinants","text":"<p>In this section we'll take a look at some properties of determinants that could make their computation simplified in some cases:</p> <ol> <li>The determinant of the identity matrix \\(\\det(I) = 1\\).</li> <li>Swapping two rows/columns of a matrix \\(A\\) reverses the sign of the \\(\\det(A)\\). More generally the sign of the determinant is given by \\((-1)^{n} \\times \\det(A)\\) where an odd number of swaps would result in a -ve sign and an even number in a +ve sign.</li> <li> <p>Determinants behave like a linear function along a row or a column. This means that</p> <ul> <li>Multiplying a row/column by a scalar \\(\\lambda\\) would also scale the determinant by \\(\\lambda\\). \\[3 . \\begin{vmatrix} 1 &amp; 2 \\\\ 4 &amp; 5 \\end{vmatrix} = \\begin{vmatrix} \\mathbf{3} &amp; \\mathbf{6} \\\\ 4 &amp; 5 \\end{vmatrix} = \\begin{vmatrix} 1 &amp; \\mathbf{6} \\\\ 4 &amp; \\mathbf{15} \\end{vmatrix} = 3 \\times -3 = -9 \\] As a consequence of this for an \\(n \\times n\\) matrix we also get that \\(\\det(\\lambda A) = \\lambda^{n} \\det(A)\\)</li> <li>A determinant of the following form can be expressed as the sum of the determinants obtained by splitting the terms of a single row/column and retaining all the others as the same. \\[\\begin{vmatrix} a + a' &amp; b + b' \\\\ c &amp; d \\\\ \\end{vmatrix} = \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\\\ \\end{vmatrix} + \\begin{vmatrix} a' &amp; b' \\\\ c &amp; d \\\\ \\end{vmatrix} \\]</li> </ul> <p>Note</p> <p>This however does not mean that \\(\\det(A + B) = \\det(A) + \\det(B)\\). This would be a wrong interpretation!</p> </li> <li> <p>If two rows/columns of a matrix \\(A\\) are the same then \\(det(A) = 0\\)</p> </li> <li>Determinants are invariant to elementary row or column operations. In other words adding or subtracting a scalar multiple of a row/column to/from another doesn't change the value of \\(\\det(A)\\). \\[\\begin{vmatrix} 3 &amp; 7 \\\\ -2 &amp; 2 \\\\ \\end{vmatrix} = \\begin{vmatrix} 3 + 0.5(7) &amp; 7 \\\\ -2 + 0.5(2) &amp; 2 \\\\ \\end{vmatrix} = \\begin{vmatrix} 6.5 &amp; 7 \\\\ -1 &amp; 2 \\\\ \\end{vmatrix} = 20 \\]</li> <li>A matrix with one or more rows of zeros will have a determinant of 0.</li> <li>The determinant of an upper triangular matrix (also works for lower) ie., a matrix with all zeros below it's main diagonal is given by the product of it's diagonal elements, also called it's pivot elements. \\[\\begin{vmatrix} \\rlap{\\kern .24em 1}\\raise .04em{\\bigcirc} &amp; 2 &amp; 3 \\\\ 0 &amp; \\rlap{\\kern .24em 5}\\raise .04em{\\bigcirc} &amp; 6 \\\\ 0 &amp; 0 &amp; \\rlap{\\kern .24em 9}\\raise .04em{\\bigcirc} \\end{vmatrix} = 1 \\times 5 \\times 9 = 45 \\]     As we'll see in later weeks, any (invertible) matrix can be reduced into it's corresponding upper triagular form using a method called Gauss elimination. Thus elimination is another way of finding determinants.</li> <li>A non-invertible matrix also known as a singular matrix has a zero determinant. Conversely a matrix \\(A\\) with \\(\\det(A) = 0\\) is singular.</li> <li>If \\(A\\) is non-singular then \\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\).</li> <li>The determinant of a matrix product is the product of the individual determinants, \\(\\det(AB) = \\det(A)\\det(B)\\).</li> <li>Determinants are invariant to transpositions ie., \\(\\det(A^T) = \\det(A)\\).</li> </ol>"},{"location":"foundational/maths_2/week_1/6_properties_determinants/#recommended-watches","title":"Recommended watches","text":""},{"location":"foundational/maths_2/week_2/1_cramers_rule/","title":"Cramer's Rule","text":""}]}